{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rice', 'maize', 'chickpea', 'kidneybeans', 'pigeonpeas',\n",
       "       'mothbeans', 'mungbean', 'blackgram', 'lentil', 'pomegranate',\n",
       "       'banana', 'mango', 'grapes', 'watermelon', 'muskmelon', 'apple',\n",
       "       'orange', 'papaya', 'coconut', 'cotton', 'jute', 'coffee'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/Crop_recommendation.csv\")\n",
    "df.head(n=2)\n",
    "\n",
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "145\n",
      "205\n",
      "43.67549305\n",
      "99.98187601\n",
      "9.93509073\n",
      "298.5601175\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Scaling X\n",
    "for col in X.columns:\n",
    "    col_max = X[col].abs().max()\n",
    "    print(col_max)\n",
    "    X[col] /= col_max\n",
    "\n",
    "# Label Encoding y\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(np.array(X), dtype=torch.float32).to(device)\n",
    "        self.y = torch.tensor(np.array(y).reshape(-1), dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ClassificationDataset(X_train, y_train)\n",
    "val = ClassificationDataset(X_val, y_val)\n",
    "test = ClassificationDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_NEURONS = 10\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_layer = nn.Linear(X.shape[1], HIDDEN_NEURONS)\n",
    "        self.hidden_layer = nn.Linear(HIDDEN_NEURONS, HIDDEN_NEURONS)  \n",
    "        self.output = nn.Linear(HIDDEN_NEURONS, len(np.unique(y)))  \n",
    "\n",
    "        self.relu = nn.ReLU()  # ReLU for hidden layers\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for multi-class classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.input_layer(x))\n",
    "        x = self.relu(self.hidden_layer(x))\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 10]              80\n",
      "              ReLU-2                   [-1, 10]               0\n",
      "            Linear-3                   [-1, 10]             110\n",
      "              ReLU-4                   [-1, 10]               0\n",
      "            Linear-5                   [-1, 22]             242\n",
      "           Softmax-6                   [-1, 22]               0\n",
      "================================================================\n",
      "Total params: 432\n",
      "Trainable params: 432\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (X.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "HIDDEN_NEURONS = 10\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0908, Train Accuracy: 4.3506%\n",
      "Validation Loss: 3.0909, Validation Accuracy: 4.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0905, Train Accuracy: 4.3506%\n",
      "Validation Loss: 3.0906, Validation Accuracy: 4.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0901, Train Accuracy: 4.3506%\n",
      "Validation Loss: 3.0902, Validation Accuracy: 4.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0898, Train Accuracy: 4.0260%\n",
      "Validation Loss: 3.0895, Validation Accuracy: 5.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0887, Train Accuracy: 4.6104%\n",
      "Validation Loss: 3.0888, Validation Accuracy: 6.3636%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0880, Train Accuracy: 7.7922%\n",
      "Validation Loss: 3.0877, Validation Accuracy: 7.8788%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0866, Train Accuracy: 9.2857%\n",
      "Validation Loss: 3.0862, Validation Accuracy: 8.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0848, Train Accuracy: 11.2987%\n",
      "Validation Loss: 3.0842, Validation Accuracy: 12.7273%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0825, Train Accuracy: 13.0519%\n",
      "Validation Loss: 3.0811, Validation Accuracy: 13.6364%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0774, Train Accuracy: 11.7532%\n",
      "Validation Loss: 3.0760, Validation Accuracy: 13.6364%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0712, Train Accuracy: 12.2727%\n",
      "Validation Loss: 3.0690, Validation Accuracy: 13.6364%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0640, Train Accuracy: 12.4026%\n",
      "Validation Loss: 3.0617, Validation Accuracy: 13.0303%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0571, Train Accuracy: 13.2468%\n",
      "Validation Loss: 3.0557, Validation Accuracy: 11.5152%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0517, Train Accuracy: 9.6104%\n",
      "Validation Loss: 3.0511, Validation Accuracy: 8.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0472, Train Accuracy: 8.9610%\n",
      "Validation Loss: 3.0476, Validation Accuracy: 8.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0443, Train Accuracy: 8.9610%\n",
      "Validation Loss: 3.0450, Validation Accuracy: 8.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0413, Train Accuracy: 8.9610%\n",
      "Validation Loss: 3.0419, Validation Accuracy: 8.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0327, Train Accuracy: 8.9610%\n",
      "Validation Loss: 3.0391, Validation Accuracy: 8.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0278, Train Accuracy: 10.5844%\n",
      "Validation Loss: 3.0363, Validation Accuracy: 9.6970%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0303, Train Accuracy: 12.1429%\n",
      "Validation Loss: 3.0331, Validation Accuracy: 10.9091%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0169, Train Accuracy: 14.6753%\n",
      "Validation Loss: 3.0302, Validation Accuracy: 12.4242%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0225, Train Accuracy: 16.7532%\n",
      "Validation Loss: 3.0265, Validation Accuracy: 13.6364%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0134, Train Accuracy: 18.2468%\n",
      "Validation Loss: 3.0230, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0095, Train Accuracy: 18.4416%\n",
      "Validation Loss: 3.0196, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0059, Train Accuracy: 18.6364%\n",
      "Validation Loss: 3.0161, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 3.0022, Train Accuracy: 18.6364%\n",
      "Validation Loss: 3.0131, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9997, Train Accuracy: 18.6364%\n",
      "Validation Loss: 3.0099, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9950, Train Accuracy: 18.6364%\n",
      "Validation Loss: 3.0062, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9880, Train Accuracy: 18.6364%\n",
      "Validation Loss: 3.0023, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9862, Train Accuracy: 18.6364%\n",
      "Validation Loss: 2.9988, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9805, Train Accuracy: 18.6364%\n",
      "Validation Loss: 2.9949, Validation Accuracy: 14.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9751, Train Accuracy: 19.4156%\n",
      "Validation Loss: 2.9902, Validation Accuracy: 15.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9711, Train Accuracy: 19.2208%\n",
      "Validation Loss: 2.9855, Validation Accuracy: 16.3636%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9682, Train Accuracy: 20.8442%\n",
      "Validation Loss: 2.9816, Validation Accuracy: 16.9697%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9600, Train Accuracy: 21.6234%\n",
      "Validation Loss: 2.9768, Validation Accuracy: 20.3030%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9595, Train Accuracy: 24.9351%\n",
      "Validation Loss: 2.9717, Validation Accuracy: 21.8182%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9475, Train Accuracy: 26.4286%\n",
      "Validation Loss: 2.9659, Validation Accuracy: 22.1212%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9490, Train Accuracy: 27.2727%\n",
      "Validation Loss: 2.9618, Validation Accuracy: 22.4242%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9408, Train Accuracy: 28.2468%\n",
      "Validation Loss: 2.9558, Validation Accuracy: 23.0303%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9379, Train Accuracy: 28.1169%\n",
      "Validation Loss: 2.9505, Validation Accuracy: 23.6364%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9328, Train Accuracy: 28.4416%\n",
      "Validation Loss: 2.9460, Validation Accuracy: 25.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9280, Train Accuracy: 28.7662%\n",
      "Validation Loss: 2.9411, Validation Accuracy: 26.9697%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9176, Train Accuracy: 30.9091%\n",
      "Validation Loss: 2.9363, Validation Accuracy: 29.3939%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9119, Train Accuracy: 31.4935%\n",
      "Validation Loss: 2.9314, Validation Accuracy: 30.3030%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9028, Train Accuracy: 33.3766%\n",
      "Validation Loss: 2.9256, Validation Accuracy: 30.9091%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.9000, Train Accuracy: 35.4545%\n",
      "Validation Loss: 2.9212, Validation Accuracy: 33.0303%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8969, Train Accuracy: 35.9091%\n",
      "Validation Loss: 2.9159, Validation Accuracy: 33.6364%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8915, Train Accuracy: 37.5974%\n",
      "Validation Loss: 2.9113, Validation Accuracy: 34.8485%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8831, Train Accuracy: 37.0779%\n",
      "Validation Loss: 2.9056, Validation Accuracy: 35.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8734, Train Accuracy: 38.6364%\n",
      "Validation Loss: 2.9010, Validation Accuracy: 35.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8757, Train Accuracy: 37.1429%\n",
      "Validation Loss: 2.8965, Validation Accuracy: 35.4545%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8690, Train Accuracy: 38.6364%\n",
      "Validation Loss: 2.8909, Validation Accuracy: 35.4545%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8616, Train Accuracy: 38.5065%\n",
      "Validation Loss: 2.8864, Validation Accuracy: 36.0606%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8583, Train Accuracy: 38.9610%\n",
      "Validation Loss: 2.8805, Validation Accuracy: 36.6667%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8529, Train Accuracy: 39.2208%\n",
      "Validation Loss: 2.8745, Validation Accuracy: 36.9697%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8391, Train Accuracy: 40.1948%\n",
      "Validation Loss: 2.8706, Validation Accuracy: 37.8788%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8401, Train Accuracy: 40.3896%\n",
      "Validation Loss: 2.8663, Validation Accuracy: 37.8788%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8370, Train Accuracy: 40.0000%\n",
      "Validation Loss: 2.8624, Validation Accuracy: 38.1818%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8325, Train Accuracy: 41.1039%\n",
      "Validation Loss: 2.8582, Validation Accuracy: 37.8788%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8214, Train Accuracy: 42.2078%\n",
      "Validation Loss: 2.8529, Validation Accuracy: 38.4848%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8194, Train Accuracy: 42.7273%\n",
      "Validation Loss: 2.8481, Validation Accuracy: 37.2727%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8162, Train Accuracy: 43.3766%\n",
      "Validation Loss: 2.8422, Validation Accuracy: 38.1818%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8100, Train Accuracy: 43.6364%\n",
      "Validation Loss: 2.8388, Validation Accuracy: 38.1818%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8046, Train Accuracy: 44.2857%\n",
      "Validation Loss: 2.8352, Validation Accuracy: 38.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8060, Train Accuracy: 43.6364%\n",
      "Validation Loss: 2.8303, Validation Accuracy: 38.1818%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.8035, Train Accuracy: 44.4805%\n",
      "Validation Loss: 2.8270, Validation Accuracy: 38.1818%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7907, Train Accuracy: 44.2857%\n",
      "Validation Loss: 2.8223, Validation Accuracy: 39.0909%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7938, Train Accuracy: 44.2208%\n",
      "Validation Loss: 2.8186, Validation Accuracy: 38.7879%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7841, Train Accuracy: 45.2597%\n",
      "Validation Loss: 2.8161, Validation Accuracy: 40.0000%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7851, Train Accuracy: 45.3247%\n",
      "Validation Loss: 2.8119, Validation Accuracy: 40.0000%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7828, Train Accuracy: 45.8442%\n",
      "Validation Loss: 2.8085, Validation Accuracy: 39.6970%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7776, Train Accuracy: 45.7143%\n",
      "Validation Loss: 2.8050, Validation Accuracy: 40.6061%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7697, Train Accuracy: 45.7792%\n",
      "Validation Loss: 2.8016, Validation Accuracy: 41.2121%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7722, Train Accuracy: 46.0390%\n",
      "Validation Loss: 2.7979, Validation Accuracy: 40.3030%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7728, Train Accuracy: 45.9740%\n",
      "Validation Loss: 2.7965, Validation Accuracy: 41.5152%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7618, Train Accuracy: 47.2078%\n",
      "Validation Loss: 2.7935, Validation Accuracy: 42.1212%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7601, Train Accuracy: 47.4675%\n",
      "Validation Loss: 2.7901, Validation Accuracy: 41.8182%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7514, Train Accuracy: 46.6234%\n",
      "Validation Loss: 2.7872, Validation Accuracy: 41.5152%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7564, Train Accuracy: 47.4675%\n",
      "Validation Loss: 2.7856, Validation Accuracy: 43.3333%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7550, Train Accuracy: 48.5714%\n",
      "Validation Loss: 2.7816, Validation Accuracy: 44.5455%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7469, Train Accuracy: 47.7922%\n",
      "Validation Loss: 2.7787, Validation Accuracy: 45.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7414, Train Accuracy: 49.4156%\n",
      "Validation Loss: 2.7763, Validation Accuracy: 46.9697%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7426, Train Accuracy: 50.1948%\n",
      "Validation Loss: 2.7715, Validation Accuracy: 46.6667%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7376, Train Accuracy: 51.1039%\n",
      "Validation Loss: 2.7686, Validation Accuracy: 45.7576%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7397, Train Accuracy: 51.2338%\n",
      "Validation Loss: 2.7655, Validation Accuracy: 46.0606%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7355, Train Accuracy: 50.7143%\n",
      "Validation Loss: 2.7606, Validation Accuracy: 45.7576%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7243, Train Accuracy: 50.8442%\n",
      "Validation Loss: 2.7567, Validation Accuracy: 45.7576%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7233, Train Accuracy: 51.8182%\n",
      "Validation Loss: 2.7513, Validation Accuracy: 46.6667%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7216, Train Accuracy: 52.1429%\n",
      "Validation Loss: 2.7462, Validation Accuracy: 47.8788%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7218, Train Accuracy: 51.9481%\n",
      "Validation Loss: 2.7425, Validation Accuracy: 48.4848%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7125, Train Accuracy: 52.7922%\n",
      "Validation Loss: 2.7397, Validation Accuracy: 50.6061%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7125, Train Accuracy: 54.8052%\n",
      "Validation Loss: 2.7330, Validation Accuracy: 53.0303%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7080, Train Accuracy: 55.0649%\n",
      "Validation Loss: 2.7287, Validation Accuracy: 53.3333%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7107, Train Accuracy: 56.8182%\n",
      "Validation Loss: 2.7242, Validation Accuracy: 55.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.7065, Train Accuracy: 55.4545%\n",
      "Validation Loss: 2.7188, Validation Accuracy: 56.6667%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.6998, Train Accuracy: 56.2338%\n",
      "Validation Loss: 2.7128, Validation Accuracy: 55.1515%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.6993, Train Accuracy: 59.4805%\n",
      "Validation Loss: 2.7102, Validation Accuracy: 56.6667%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.6954, Train Accuracy: 57.4026%\n",
      "Validation Loss: 2.7081, Validation Accuracy: 56.3636%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.6917, Train Accuracy: 56.2987%\n",
      "Validation Loss: 2.7050, Validation Accuracy: 57.2727%\n",
      "==================================================\n",
      "Epoch 1: Batch Size: 32, LR: 0.001, Optimizer: adam\n",
      "Train Loss: 2.6875, Train Accuracy: 56.6234%\n",
      "Validation Loss: 2.7020, Validation Accuracy: 57.2727%\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [3.0908,\n",
       "  3.0905,\n",
       "  3.0901,\n",
       "  3.0898,\n",
       "  3.0887,\n",
       "  3.088,\n",
       "  3.0866,\n",
       "  3.0848,\n",
       "  3.0825,\n",
       "  3.0774,\n",
       "  3.0712,\n",
       "  3.064,\n",
       "  3.0571,\n",
       "  3.0517,\n",
       "  3.0472,\n",
       "  3.0443,\n",
       "  3.0413,\n",
       "  3.0327,\n",
       "  3.0278,\n",
       "  3.0303,\n",
       "  3.0169,\n",
       "  3.0225,\n",
       "  3.0134,\n",
       "  3.0095,\n",
       "  3.0059,\n",
       "  3.0022,\n",
       "  2.9997,\n",
       "  2.995,\n",
       "  2.988,\n",
       "  2.9862,\n",
       "  2.9805,\n",
       "  2.9751,\n",
       "  2.9711,\n",
       "  2.9682,\n",
       "  2.96,\n",
       "  2.9595,\n",
       "  2.9475,\n",
       "  2.949,\n",
       "  2.9408,\n",
       "  2.9379,\n",
       "  2.9328,\n",
       "  2.928,\n",
       "  2.9176,\n",
       "  2.9119,\n",
       "  2.9028,\n",
       "  2.9,\n",
       "  2.8969,\n",
       "  2.8915,\n",
       "  2.8831,\n",
       "  2.8734,\n",
       "  2.8757,\n",
       "  2.869,\n",
       "  2.8616,\n",
       "  2.8583,\n",
       "  2.8529,\n",
       "  2.8391,\n",
       "  2.8401,\n",
       "  2.837,\n",
       "  2.8325,\n",
       "  2.8214,\n",
       "  2.8194,\n",
       "  2.8162,\n",
       "  2.81,\n",
       "  2.8046,\n",
       "  2.806,\n",
       "  2.8035,\n",
       "  2.7907,\n",
       "  2.7938,\n",
       "  2.7841,\n",
       "  2.7851,\n",
       "  2.7828,\n",
       "  2.7776,\n",
       "  2.7697,\n",
       "  2.7722,\n",
       "  2.7728,\n",
       "  2.7618,\n",
       "  2.7601,\n",
       "  2.7514,\n",
       "  2.7564,\n",
       "  2.755,\n",
       "  2.7469,\n",
       "  2.7414,\n",
       "  2.7426,\n",
       "  2.7376,\n",
       "  2.7397,\n",
       "  2.7355,\n",
       "  2.7243,\n",
       "  2.7233,\n",
       "  2.7216,\n",
       "  2.7218,\n",
       "  2.7125,\n",
       "  2.7125,\n",
       "  2.708,\n",
       "  2.7107,\n",
       "  2.7065,\n",
       "  2.6998,\n",
       "  2.6993,\n",
       "  2.6954,\n",
       "  2.6917,\n",
       "  2.6875],\n",
       " 'val_loss': [3.0909,\n",
       "  3.0906,\n",
       "  3.0902,\n",
       "  3.0895,\n",
       "  3.0888,\n",
       "  3.0877,\n",
       "  3.0862,\n",
       "  3.0842,\n",
       "  3.0811,\n",
       "  3.076,\n",
       "  3.069,\n",
       "  3.0617,\n",
       "  3.0557,\n",
       "  3.0511,\n",
       "  3.0476,\n",
       "  3.045,\n",
       "  3.0419,\n",
       "  3.0391,\n",
       "  3.0363,\n",
       "  3.0331,\n",
       "  3.0302,\n",
       "  3.0265,\n",
       "  3.023,\n",
       "  3.0196,\n",
       "  3.0161,\n",
       "  3.0131,\n",
       "  3.0099,\n",
       "  3.0062,\n",
       "  3.0023,\n",
       "  2.9988,\n",
       "  2.9949,\n",
       "  2.9902,\n",
       "  2.9855,\n",
       "  2.9816,\n",
       "  2.9768,\n",
       "  2.9717,\n",
       "  2.9659,\n",
       "  2.9618,\n",
       "  2.9558,\n",
       "  2.9505,\n",
       "  2.946,\n",
       "  2.9411,\n",
       "  2.9363,\n",
       "  2.9314,\n",
       "  2.9256,\n",
       "  2.9212,\n",
       "  2.9159,\n",
       "  2.9113,\n",
       "  2.9056,\n",
       "  2.901,\n",
       "  2.8965,\n",
       "  2.8909,\n",
       "  2.8864,\n",
       "  2.8805,\n",
       "  2.8745,\n",
       "  2.8706,\n",
       "  2.8663,\n",
       "  2.8624,\n",
       "  2.8582,\n",
       "  2.8529,\n",
       "  2.8481,\n",
       "  2.8422,\n",
       "  2.8388,\n",
       "  2.8352,\n",
       "  2.8303,\n",
       "  2.827,\n",
       "  2.8223,\n",
       "  2.8186,\n",
       "  2.8161,\n",
       "  2.8119,\n",
       "  2.8085,\n",
       "  2.805,\n",
       "  2.8016,\n",
       "  2.7979,\n",
       "  2.7965,\n",
       "  2.7935,\n",
       "  2.7901,\n",
       "  2.7872,\n",
       "  2.7856,\n",
       "  2.7816,\n",
       "  2.7787,\n",
       "  2.7763,\n",
       "  2.7715,\n",
       "  2.7686,\n",
       "  2.7655,\n",
       "  2.7606,\n",
       "  2.7567,\n",
       "  2.7513,\n",
       "  2.7462,\n",
       "  2.7425,\n",
       "  2.7397,\n",
       "  2.733,\n",
       "  2.7287,\n",
       "  2.7242,\n",
       "  2.7188,\n",
       "  2.7128,\n",
       "  2.7102,\n",
       "  2.7081,\n",
       "  2.705,\n",
       "  2.702],\n",
       " 'train_acc': [4.3506,\n",
       "  4.3506,\n",
       "  4.3506,\n",
       "  4.026,\n",
       "  4.6104,\n",
       "  7.7922,\n",
       "  9.2857,\n",
       "  11.2987,\n",
       "  13.0519,\n",
       "  11.7532,\n",
       "  12.2727,\n",
       "  12.4026,\n",
       "  13.2468,\n",
       "  9.6104,\n",
       "  8.961,\n",
       "  8.961,\n",
       "  8.961,\n",
       "  8.961,\n",
       "  10.5844,\n",
       "  12.1429,\n",
       "  14.6753,\n",
       "  16.7532,\n",
       "  18.2468,\n",
       "  18.4416,\n",
       "  18.6364,\n",
       "  18.6364,\n",
       "  18.6364,\n",
       "  18.6364,\n",
       "  18.6364,\n",
       "  18.6364,\n",
       "  18.6364,\n",
       "  19.4156,\n",
       "  19.2208,\n",
       "  20.8442,\n",
       "  21.6234,\n",
       "  24.9351,\n",
       "  26.4286,\n",
       "  27.2727,\n",
       "  28.2468,\n",
       "  28.1169,\n",
       "  28.4416,\n",
       "  28.7662,\n",
       "  30.9091,\n",
       "  31.4935,\n",
       "  33.3766,\n",
       "  35.4545,\n",
       "  35.9091,\n",
       "  37.5974,\n",
       "  37.0779,\n",
       "  38.6364,\n",
       "  37.1429,\n",
       "  38.6364,\n",
       "  38.5065,\n",
       "  38.961,\n",
       "  39.2208,\n",
       "  40.1948,\n",
       "  40.3896,\n",
       "  40.0,\n",
       "  41.1039,\n",
       "  42.2078,\n",
       "  42.7273,\n",
       "  43.3766,\n",
       "  43.6364,\n",
       "  44.2857,\n",
       "  43.6364,\n",
       "  44.4805,\n",
       "  44.2857,\n",
       "  44.2208,\n",
       "  45.2597,\n",
       "  45.3247,\n",
       "  45.8442,\n",
       "  45.7143,\n",
       "  45.7792,\n",
       "  46.039,\n",
       "  45.974,\n",
       "  47.2078,\n",
       "  47.4675,\n",
       "  46.6234,\n",
       "  47.4675,\n",
       "  48.5714,\n",
       "  47.7922,\n",
       "  49.4156,\n",
       "  50.1948,\n",
       "  51.1039,\n",
       "  51.2338,\n",
       "  50.7143,\n",
       "  50.8442,\n",
       "  51.8182,\n",
       "  52.1429,\n",
       "  51.9481,\n",
       "  52.7922,\n",
       "  54.8052,\n",
       "  55.0649,\n",
       "  56.8182,\n",
       "  55.4545,\n",
       "  56.2338,\n",
       "  59.4805,\n",
       "  57.4026,\n",
       "  56.2987,\n",
       "  56.6234],\n",
       " 'val_acc': [4.8485,\n",
       "  4.8485,\n",
       "  4.8485,\n",
       "  5.1515,\n",
       "  6.3636,\n",
       "  7.8788,\n",
       "  8.7879,\n",
       "  12.7273,\n",
       "  13.6364,\n",
       "  13.6364,\n",
       "  13.6364,\n",
       "  13.0303,\n",
       "  11.5152,\n",
       "  8.7879,\n",
       "  8.7879,\n",
       "  8.7879,\n",
       "  8.7879,\n",
       "  8.7879,\n",
       "  9.697,\n",
       "  10.9091,\n",
       "  12.4242,\n",
       "  13.6364,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  14.8485,\n",
       "  15.1515,\n",
       "  16.3636,\n",
       "  16.9697,\n",
       "  20.303,\n",
       "  21.8182,\n",
       "  22.1212,\n",
       "  22.4242,\n",
       "  23.0303,\n",
       "  23.6364,\n",
       "  25.1515,\n",
       "  26.9697,\n",
       "  29.3939,\n",
       "  30.303,\n",
       "  30.9091,\n",
       "  33.0303,\n",
       "  33.6364,\n",
       "  34.8485,\n",
       "  35.1515,\n",
       "  35.1515,\n",
       "  35.4545,\n",
       "  35.4545,\n",
       "  36.0606,\n",
       "  36.6667,\n",
       "  36.9697,\n",
       "  37.8788,\n",
       "  37.8788,\n",
       "  38.1818,\n",
       "  37.8788,\n",
       "  38.4848,\n",
       "  37.2727,\n",
       "  38.1818,\n",
       "  38.1818,\n",
       "  38.7879,\n",
       "  38.1818,\n",
       "  38.1818,\n",
       "  39.0909,\n",
       "  38.7879,\n",
       "  40.0,\n",
       "  40.0,\n",
       "  39.697,\n",
       "  40.6061,\n",
       "  41.2121,\n",
       "  40.303,\n",
       "  41.5152,\n",
       "  42.1212,\n",
       "  41.8182,\n",
       "  41.5152,\n",
       "  43.3333,\n",
       "  44.5455,\n",
       "  45.1515,\n",
       "  46.9697,\n",
       "  46.6667,\n",
       "  45.7576,\n",
       "  46.0606,\n",
       "  45.7576,\n",
       "  45.7576,\n",
       "  46.6667,\n",
       "  47.8788,\n",
       "  48.4848,\n",
       "  50.6061,\n",
       "  53.0303,\n",
       "  53.3333,\n",
       "  55.1515,\n",
       "  56.6667,\n",
       "  55.1515,\n",
       "  56.6667,\n",
       "  56.3636,\n",
       "  57.2727,\n",
       "  57.2727]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(model, train_dataset, val_dataset, criterion, epochs_config):\n",
    "    total_loss_train_plot = []\n",
    "    total_loss_validation_plot = []\n",
    "    total_acc_train_plot = []\n",
    "    total_acc_validation_plot = []\n",
    "    \n",
    "    for epoch, config in enumerate(epochs_config):\n",
    "        for e in range(EPOCHS):\n",
    "            batch_size = config.get(\"batch_size\", 32)\n",
    "            learning_rate = config.get(\"learning_rate\", 0.001)\n",
    "            optimizer_type = config.get(\"optimizer\", \"adam\").lower()\n",
    "            \n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            if optimizer_type == \"adam\":\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            elif optimizer_type == \"sgd\":\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported optimizer: {optimizer_type}\")\n",
    "            \n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            ## Training Loop\n",
    "            for data in train_dataloader:\n",
    "                inputs, labels = data\n",
    "                labels = labels.long()\n",
    "                prediction = model(inputs)\n",
    "                batch_loss = criterion(prediction, labels)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (prediction.argmax(dim=1) == labels).sum().item()\n",
    "                total_acc_train += acc\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            ## Validation Loop\n",
    "            with torch.no_grad():\n",
    "                for data in val_dataloader:\n",
    "                    inputs, labels = data\n",
    "                    labels = labels.long()\n",
    "                    prediction = model(inputs)\n",
    "                    batch_loss = criterion(prediction, labels)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (prediction.argmax(dim=1) == labels).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            total_loss_train_plot.append(round(total_loss_train / len(train_dataloader), 4))\n",
    "            total_loss_validation_plot.append(round(total_loss_val / len(val_dataloader), 4))\n",
    "            total_acc_train_plot.append(round(total_acc_train / len(train_dataset) * 100, 4))\n",
    "            total_acc_validation_plot.append(round(total_acc_val / len(val_dataset) * 100, 4))\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}: Batch Size: {batch_size}, LR: {learning_rate}, Optimizer: {optimizer_type}\")\n",
    "            print(f\"Train Loss: {total_loss_train / len(train_dataloader):.4f}, Train Accuracy: {total_acc_train / len(train_dataset) * 100:.4f}%\")\n",
    "            print(f\"Validation Loss: {total_loss_val / len(val_dataloader):.4f}, Validation Accuracy: {total_acc_val / len(val_dataset) * 100:.4f}%\")\n",
    "            print(\"=\" * 50)\n",
    "        \n",
    "        return {\n",
    "            \"train_loss\": total_loss_train_plot,\n",
    "            \"val_loss\": total_loss_validation_plot,\n",
    "            \"train_acc\": total_acc_train_plot,\n",
    "            \"val_acc\": total_acc_validation_plot\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "epochs_config = [\n",
    "    {\"batch_size\": 32, \"learning_rate\": 0.001, \"optimizer\": \"adam\"},\n",
    "    {\"batch_size\": 64, \"learning_rate\": 0.0005, \"optimizer\": \"sgd\"},\n",
    "    {\"batch_size\": 128, \"learning_rate\": 0.0001, \"optimizer\": \"adam\"}\n",
    "]\n",
    "\n",
    "\n",
    "train_model(model, train, val, criterion, epochs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_loss_train_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m fig, axs = plt.subplots(nrows=\u001b[32m1\u001b[39m, ncols=\u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m axs[\u001b[32m0\u001b[39m].plot(\u001b[43mtotal_loss_train_plot\u001b[49m, label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m axs[\u001b[32m0\u001b[39m].plot(total_loss_validation_plot, label=\u001b[33m'\u001b[39m\u001b[33mValidation Loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m axs[\u001b[32m0\u001b[39m].set_title(\u001b[33m'\u001b[39m\u001b[33mTraining and Validation Loss over Epochs\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'total_loss_train_plot' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "axs[0].plot(total_loss_train_plot, label='Training Loss')\n",
    "axs[0].plot(total_loss_validation_plot, label='Validation Loss')\n",
    "axs[0].set_title('Training and Validation Loss over Epochs')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].set_ylim([0, 2])\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(total_acc_train_plot, label='Training Accuracy')\n",
    "axs[1].plot(total_acc_validation_plot, label='Validation Accuracy')\n",
    "axs[1].set_title('Training and Validation Accuracy over Epochs')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_ylim([0, 100])\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is: 86.97%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  total_loss_test = 0\n",
    "  total_acc_test = 0\n",
    "  for data in test_dataloader:\n",
    "    inputs, labels = data\n",
    "    labels = labels.long()\n",
    "    prediction = model(inputs).squeeze(1)\n",
    "\n",
    "    batch_loss_test = criterion((prediction), labels)\n",
    "    total_loss_test += batch_loss_test.item()\n",
    "    acc = ((prediction).round() == labels).sum().item()\n",
    "    total_acc_test += acc\n",
    "\n",
    "print(f\"Accuracy Score is: {round((total_acc_test/X_test.shape[0])*100, 2)}%\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/tabular_sowing_classification.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_state_dict(torch.load(\"./models/tabular_sowing_classification.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_fasal_guru",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
